% !TEX encoding = UTF-8
% !TEX TS-program = pdflatex
% !TEX root = ../thesis.tex

%**************************************************************
\chapter{METODOLOGIA}
\label{Capitolo3}
\thispagestyle{empty}

Nel capitolo precedente sono stati spiegati tutti i concetti principali che 
compongono un sistema di visione artificiale, il quale ha lo scopo principale 
di effettuare la comprensione della scena mediante l'utilizzo di tecniche di 
object detection e di semantic segmentation. Oltre a questi concetti, sono 
state definite anche le comuni tecniche di compressione/ottimizzazione che 
permettono ad un modello di essere eseguito anche su dispositivi a limitate 
risorse computazionali, relativamente economico, alla portata di tutti. 
L'obiettivo finale dello studio è basato sull'incremento delle prestazioni di 
un modello tramite l'utilizzo di una delle tecniche di compressione/ottimizzazione 
citate nel capitolo precedente. Nel seguente capitolo vengono 
riportate tutte le metodologie adottate che hanno portato alla realizzazione 
di un metodo personalizzato avente lo scopo prefissato. Il focus principale 
sarà rivolto verso la tecnica di object detection. È proprio quest'ultima 
tecnica ad essere stata utilizzata maggiormente in questo elaborato. Le
risorse computazionali richieste da codesta risultano essere onerose. Essendo 
un sistema autonomo implementato all'interno di una centralina dedicata, 
bisogna aver un chiaro prospetto delle potenzialità richieste da un modello di 
visione artificiale per poter raggiungere l'obiettivo finale. Il dispositivo preso 
in riferimento è costituito da un nota scheda di elaborazione embedded, che 
prende il nome di Nvidia Jetson Nano. Le potenzialità messe a disposizione 
da questa scheda sono state comparate, in termini di Frames-per-Second 
(FPS), con quelle messe a disposizione sia dal computer del sottoscritto 
che da Google Colaboratory (Colab). Avendo caratteristiche hardware ben 
differenti l'uno dall'altro, si è pensato di creare una comparazione standard 
composta dallo stesso codice eseguito su tutte e tre le diverse architetture. 
Dopo aver ottenuto i primi risultati dai modelli pre-addestrati, messi gentilmente 
a disposizione da NVidia, questi hanno costituito le baselines ovvero 
i punti di riferimento da cui partire. Per poter ricavare i benchmarks, 
tutti i modelli, pre-addestrati e proposto, saranno sottoposti al percorso di 
elaborazione raffigurato in Figura (\ref{flow_chart}). Per la visualizzazione dei risultati 
ottenuti in ogni test, si rimanda la lettura al capitolo (\ref{Chapter4}).
\begin{figure}
    \centering
    \includegraphics[width = \linewidth]{flow_chart.png}
    \centering
    \caption{Flusso di esecuzione di ogni modello.}
    \label{flow_chart}
\end{figure}

\section{NVidia Jetson Nano}
La Jetson Nano (B01), presentata nel Marzo del 2019,  è una scheda embedded 
sviluppata da NVidia che rappresenta il prodotto più piccolo della 
famiglia Jetson. L'utilizzo della scheda è rivolto principalmente verso varie 
applicazioni di intelligenza artificiale, visione artificiale e robotica. A bordo 
troviamo un processore e una scheda madre che offre una potenza di calcolo 
pari a 128 Cuda cores. L'obiettivo di questa scheda è quello di funzionare 
con reti neurali e offrire le migliori prestazioni quando viene utilizzata per 
eseguire inferenze. A differenza di altre architetture, lo Jetson Nano utilizza 
una precisione Floating point (FP) a 16-bit che lo rende competitivo rispetto 
ad altri device embedded. Purtroppo non supporta la precisione a 8-bit 
ma è comunque in grado di lavorare con qualsiasi rete disponibile e con 
qualsiasi framework di deep learning popolare (es: Pytorch, TensorFlow, 
Keras, Caffe etc.). In questo dispositivo è possibile effettuare sia il rilascio 
(deploy) dell'applicazione che l'addestramento della rete ma, in quest'ultimo 
caso, risulta essere lento a causa delle prestazioni computazionali ridotte. 
Risulta inoltre possibile effetuare operazioni di transfer learning tra i modelli.
Oltre ad avere il vantaggio delle dimensioni ridotte, un altro principale vantaggio 
della Jetson Nano deriva dall'applicazione dell'acceleratore TensorRT. 
Quest'ultimo esegue un processo di quantizzazione che è utile a convertire i 
pesi e gli input in precisioni Floating Point inferiori, in modo da preservare 
la memoria che, su una scheda del genere, rappresenta una limitazione. A 
tal proposito, il dispositivo non fornisce alcun tipo di memoria integrata, 
ma esiste la possibilità di aggiungerne una grazie alla presenza di uno slot 
di espansione in cui è possibile alloggiare una scheda micro-sd. Essendo una 
scheda embedded, a differenza di altri computer che utilizzano alimentatori 
da diversi Watts (W), la Jetson Nano può utilizzare due diversi livelli di 
wattaggio. Il primo, quello da 5W, raggiungibile grazie alla presenza di 
una porta micro-usb, mentre il secondo, quello da 10W, è raggiungibile solo 
grazie all'utilizzo di un alimentatore esterno collegato tramite l'ingresso 
jack. Il massimo livello di performance, raggiungibile dalla GPU, avviene 
proprio tramite l'utilizzo dell'alimentatore esterno. Per rendere l'idea delle 
dimensioni e dell'intera architettura, in Figura (\ref{jetson}) è riportata la Jetson 
Nano.
\begin{figure}[]
    \begin{minipage}[t]{.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{jetson1.png}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{.45\textwidth}
        \centering
        \includegraphics[width= 0.8\textwidth]{jetson2.png}
    \end{minipage}  
    \caption{NVidia Jetson Nano.}
    \label{jetson}
\end{figure}

\section{Modelli}
Il lavoro di tesi è incentrato nello studio di diversi modelli profondi e nell'applicazione, del tutto personalizzata, di una nota tecnica di compressione/ottimizzazione su uno di questi. Lo scopo è quello di aumentare la velocità di inferenza, nonché i Frames-Per-Second (FPS), in modo da permettere la sua implementazione su diverse architetture anche con limitate risorse computazionali (es: Jetson Nano). Per arrivare all'obiettivo finale, si è partito dallo studio delle performance ottenute da diversi modelli pre-addestrati messi a disposizione dalla community di NVidia. Dopo aver ottenuto i risultati, lo studio si è incentrato sull'utilizzo di una particolare architettura di rete, nota con il nome di \emph{MobileNet-V1}, con l'obiettivo di migliorarla. Successivamente il focus si è spostato nell'implementazione di tale rete in una seconda architettura, quest'ultima avente il nome di \emph{Single-Shot-Detector (SSD)}. Prima di giungere ai risultati ottenuti, è fondamentale avere un'ampia visione dei modelli utilizzati e di come questi hanno evidenziato le capacità computazionali delle architetture utilizzate. 
\subsection{Modelli pre-addestrati}
I modelli messi a disposizione da NVidia hanno permesso da subito di estrarre le vere potenzialità di tutte le architetture di studio. Grazie alla presenza di uno script eseguibile nelle librerie jetson utils (ref Jetson utils), sono stati reperiti diversi modelli, allenati su diverse tipologie di datasets, divisi in due categorie: Object Detection (Tab. \ref{pre_trained_models_obj_det}) e Semantic Segmentation (Tab. \ref{pre_trained_models_sem_seg}).
Tutti i modelli riportati sono stati testati su tutte le architetture di riferimento. Nuovamente si ricorda al lettore che lo scopo della tesi è aumentare codeste performance. Il produttore ha eseguito alcuni test sulla velocità di inferenza solamente su alcuni dei modelli proposti. Per poter visualizzare i risultati ottenuti, si rimanda la lettura al capitolo successivo.
\begin{table}[]
    \renewcommand{\baselinestretch}{1}
    \centering
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{|c||L|L||}
        \hline
        \multirow{2}{*}{\bfseries{MODELLI}} & \multicolumn{2}{c||}{\bfseries{OBJECT DETECTION}}\\            & \bfseries{Dataset} & \bfseries{Risoluzione}\\
        \hline
        \hline
        {\bfseries{SSD-MOBILENET-V1}} & MS COCO & 640$\times$480\\
        \hline
        {\bfseries{SSD-MOBILENET-V2}} & MS COCO & 640$\times$480\\
        \hline 
        {\bfseries{SSD-INCEPTION-V2}} & MS COCO & 640$\times$480\\
        \hline
        {\bfseries{PEDNET}} & MS COCO & 640$\times$480\\
        \hline
        {\bfseries{MULTIPEDNET}} & MS COCO & 640$\times$480\\
        \hline
    \end{tabular}
    \end{adjustbox}
    \vspace{0.5cm}
    \caption{Modelli pre-addestrati utilizzati per l'attività di object detection.}
    \label{pre_trained_models_obj_det}
\end{table}

\begin{table}[]
    \renewcommand{\baselinestretch}{1}
    \centering
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{|c||L|L||}
        \hline
        \multirow{2}{*}{\bfseries{MODELLI}} & \multicolumn{2}{c||}{\bfseries{SEMANTIC SEGMENTATION}}\\            & \bfseries{Dataset} & \bfseries{Risoluzione}\\
        \hline
        \hline
        {\bfseries{FCN-RESNET-18}} & Cityscapes & 512$\times$256\\
        \hline
        {\bfseries{FCN-RESNET-18}} & Cityscapes & 1024$\times$512\\
        \hline 
        {\bfseries{FCN-RESNET-18}} & Cityscapes & 2048$\times$1024\\
        \hline
        {\bfseries{FCN-RESNET-18}} & Pascal Voc & 320$\times$320\\
        \hline
        {\bfseries{FCN-RESNET-18}} & Pascal Voc & 512$\times$320\\
        \hline
    \end{tabular}
    \end{adjustbox}
    \vspace{0.5cm}
    \caption{Modelli pre-addestrati utilizzati per l'attività di semantic segmentation.}
    \label{pre_trained_models_sem_seg}
\end{table}

\subsection{Modello base di riferimento}
Per verificare la veridicità dei risultati ottenuti e quelli dichiarati dal produttore, l'intero lavoro di tesi si è concentrato maggiormente nella ricerca e nella costruzione di un modello "from scratch". Per aver un confronto equo, dopo diversi studi focalizzati sulle varie architetture dei modelli pre-addestrati, la scelta è ricaduta sull'implementazione della rete \emph{MobileNet-V1}. Questa decisione è motivata nelle sezioni più avanti quando si discuterà delle tecniche di compressione/ottimizzazione adottate. 
Approfondendo il paper scientifico \cite{howard2017mobilenets}, o scopo degli autori è quello di creare un'architettura di rete dinamica, influenzata dalla presenza di diversi iper-parametri, adattabile in diversi dispositivi compresi quelli mobili. L'architettura della rete MobileNet-V1 è raffigurata in Figura \ref{mobilenetV1}:
\begin{figure}
    \centering
    \includegraphics[width = 0.9\linewidth]{mobilenet_architecture.png}
    \centering
    \caption{Architettura MobileNet-V1.}
    \label{mobilenetV1}
\end{figure}
La particolarità che contraddistingue la seguente rete dalle altre è basata sulla presenza di convoluzioni personalizzate chiamate "\emph{Depthwise Separable Convolutions}"  che, a differenza delle comuni convoluzioni, fattorizzano una convoluzione standard prima in una convoluzione profonda e successivamente in una convoluzione 1x1 chiamata "pointwise convolution", formando due layers distinti (Fig. \ref{depth_wise}).
\begin{figure}
    \centering
    \includegraphics[width = 0.8\linewidth]{depth_wise.png}
    \centering
    \caption{La convoluzione standard (a) viene sostituita da due layers: depthwise convolution (b) e pointwise convolution (c) per costruire un depthwise separable filter.}
    \label{depth_wise}
\end{figure}
La prima convoluzione ha l'obiettivo di applicare un singolo filtro per ogni canale in input, mentre la seconda creare una combinazione lineare dell'output. Lo scopo della fattorizzazione è quello di ridurre drasticamente la computazione e le dimensioni del modello. Tutti i livelli sono seguiti da una normalizzazione batch e una funzione di attivazione ReLU, ad eccezione del livello finale che è un completamente connesso. Il tutto termina con una Softmax utile per la classificazione. In totale, MobileNet ha 28 livelli le costituiscono la profondità dell'intera rete. Per costruire una rete a dimensioni ridotte, con un numero di parametri inferiore, beneficiando di un incremento di velocità, gli autori hanno introdotto due tipologie di parametri, uno dei quali risultate molto importanti per raggiungere l'obiettivo di questa tesi. I  due parametri prendono il nome di: 
\begin{enumerate}
    \item \emph{Width Multiplier ($\alpha$)}
    \item \emph{Resolution Multiplier ($p$)}
\end{enumerate}
Il ruolo del width multiplier $\alpha$, un valore tra 0 e 1 compreso, è quello di ridurre le dimensioni della rete in modo uniforme su ogni strato. Per quanto riguarda l'iper-parametro Resolution multiplier $p$,  il suo scopo è quello di accelerare l'inferenza della rete andando ad agire sull'immagine in input e nella rappresentazione interna di ogni livello. In poche parole, l'intento di quest'ultimo parametro è quello di ridurre la  risoluzione dell'immagine di input, e delle successive sue elaborazioni, per poter incrementare la velocità. Il decremento portato da questo parametro fa sì che la risoluzione possa raggiungere uno dei seguenti valori $p=\{224, 192, 160, 128\}$.
Nel complesso, l'utilizzo di questa architettura ha permesso lo svolgimento di attività di object detection. 


\section{Supporto Cuda Frameworks}
\subsection{TensorRT}
\begin{figure}
    \centering
    \includegraphics[width = \linewidth]{tensorrtOpt.png}
    \centering
    \caption{Ottimizzazioni si TensorRT sui modelli.}
    \label{tensorrt}
\end{figure}
TensorRT è un framework di machine learning, sviluppato interamente da 
NVidia, che esegue cinque diverse procedure di ottimizzazione su architetture 
basate su scheda GPU NVidia (Fig. (\ref{tensorrt})). 
\begin{enumerate}
    \item {\bfseries{\emph{Precision Calibration}}}: in questa ottimizzazione viene eseguita 
    l'operazione di \emph{Quantizzazione} che permette di mappare tutti i valori 
    dei pesi da una precisione FP32 bit a FP16 bit, creando una perdita 
    di precisione trascurabile.
    \item {\bfseries{\emph{Layer \& Tensor Fusion}}}: la seconda ottimizzazione riguarda l'eliminazione 
    di tutti quei layer che non vengono utilizzati, questo è 
    utile per poter evitare calcoli inutili. Successivamente, le operazioni 
    di Convoluzione, ReLU e normalizzazione Batch, vengono fuse in un 
    unico layer (\emph{CBR}). Questa operazione permette di eseguire calcoli 
    in una maniera più veloce ed efficace. Nella Figura (\ref{fusion_tensorrt}) si possono 
    vedere meglio quali sono tutti i layer che vengono fusi da TensorRT.
    \item {\bfseries{\emph{Kernel Auto-Tuning}}}: la terza ottimizzazione viene effettuata direttamente 
    sui filtri utilizzati nella rete. Durante questa fase vengono 
    selezionati i migliori layer, algoritmi e dimensioni di batch in base alla 
    piattaforma GPU di destinazione.
    \item {\bfseries{\emph{Dynamic Tensor Memory}}}: la gestione della memoria viene effettuata 
    proprio in questa ottimizzazione. TensorRT alloca memoria 
    solo per durante il periodo di vita di un tensore scongiurando un 
    sovraccarico di allocazioni permettendo esecuzioni rapide ed efficienti.
    \item {\bfseries{\emph{Multiple Stream Execution}}}: l'ultima ottimizzazione riguarda l'elaborazione 
    parallela di multipli flussi di input. Fondamentalmente, 
    questo è possibile utilizzando la libreria CUDA stream.
\end{enumerate}
L'aspetto più importante da ricordarsi, quando si utilizza TensorRT, è che 
bisogna assicurarsi che la procedura di ottimizzazione avvenga sulla stessa 
GPU NVidia che verrà utilizzata per l'inferenza. Questo deve avvenire 
in quanto TensorRT utilizza kernel specifici a seconda della piattaforma 
di destinazione. L'utilizzo di una ottimizzazione su una differente scheda 
grafica porta alla creazione di errori in fase di inferenza. 
\begin{figure}
    \centering
    \includegraphics[width = \linewidth]{optTensor.png}
    \centering
    \caption{Fusione dei livelli Convolutional, Batch e ReLU eseguita da TensorRT.}
    \label{fusion_tensorrt}
\end{figure}

\subsection{NVidia Jetson utils}\label{utils}
NVidia dispone di una comunità che supporta l'evoluzione di tutte le sue schede 
embedded, inclusa la Jetson Nano. Questa associazione ha dato vita 
a delle librerie utilizzate in ambito di computer vision, nello specifico rivolto 
alla gestione e alla  progettazione di reti neurali. In ambito di inferenza, una 
parte di codeste utilizza l'acceleratore TensorRT per distribuire in modo 
efficiente le reti neurali sulla piattaforma Jetson utilizzata, consentendo 
un miglioramento delle prestazioni e al contempo una migliore efficienza 
energetica. Il codice sorgente messo a disposizione, sviluppato sia in linguaggio 
C++ che in Python (principalmente utilizzato in questo elaborato), 
è composto da diversi scripts che mirano ad eseguire i modelli per svolgere 
svariate attività:
\begin{itemize}
    \item \emph{ImageNet.py}: per attività di Image Recognition;
    \item \emph{DetectNet.py}: per attività di Object detection;
    \item \emph{SegNet.py}: per attività di Semantic Segmentation;
    \item \emph{PoseNet.py}: per attività di Pose Estimation.
\end{itemize}
Ognuno di questi ha lo scopo di eseguire l'inferenza di una apposita rete 
per produrre l'output inerente una specifica attività. In questa tesi sono 
stati utilizzati i primi tre scripts in quanto coerenti con lo scopo prefissato. 
L'input è costituito da uno stream di immagini, video o dati, proveniente 
da una sorgente esterna come per esempio una webcam esterna, collegata 
tramite una porta usb, oppure una webcam collegata tramite interfaccia 
CSI/ISP predisposta direttamente sulla scheda. I frame di input possono 
provenire da un file avente estensione jpeg, mp4, RTP, RTPS etc. Nel caso 
in cui l'input provenga da una fonte esterna, verrà utilizzato il protocollo 
V4L2 che imposterà il maggior numero di frame rate alla massima risoluzione 
supportata dalla fonte. Per quanto riguarda l'output, questo può 
essere distribuito nel medesimo formato di input. I codecs supportati dalla 
piattaforma sono i seguenti:
\begin{itemize}
    \item \emph{Decode}: H.264, H.265, VP8, VP9, MPEG-2, MPEG-4 e MJPEG;
    \item \emph{Encode}: H.264, H.265, VP8, VP9 e MJPEG.
\end{itemize}
Le APIs mettono a disposizione anche alcuni scripts che utilizzano il supporto 
CUDA in grado di gestire e manipolare le immagini, che siano di input o 
di output. Ritornando agli scripts principali, ImageNet accetta un'immagine 
in input e restituendone un intervallo di probabilità in output per ogni classe. 
DetectNet, a differenza di ImageNet, oltre a permette di concentrarsi sul rilevamento 
di oggetti, specifica la loro posizione tramite delle bounding boxes, 
all'interno del frame in input. Rispetto alla classificazione delle immagini, 
le reti utilizzate in questo contesto sono in grado di rilevare multipli oggetti, 
appartenenti alla stessa categoria e non, nell'input specificato. L'output 
prodotto è rappresentato da delle coordinate utili a delimitare i riquadri che 
contraddistinguono ogni singolo oggetto di ogni classe (Fig. \ref{detectnet_result}).
\begin{figure}
    \centering
    \includegraphics[width = \linewidth]{detectnet_result.png}
    \centering
    \caption{Esempio di ouput prodotto da DetectNet sulla Jetson Nano.}
    \label{detectnet_result}
\end{figure}
Per quanto 
riguarda l'attività di segmentazione semantica, questa viene interamente 
svolta dallo script Python Segnet. L'output prodotto da quest'ultimo si 
basa in un'immagine in cui vi è applicata una maschera sovrapposta utile a 
classificare ogni singolo pixel presente nell'immagine di input. Ogni pixel 
della maschera corrisponderà alla classe dell'oggetto sottostante classificato 
(Fig. \ref{segnet_result}).
\begin{figure}
    \centering
    \includegraphics[width = \linewidth]{segnet_result.png}
    \centering
    \caption{Esempio di ouput prodotto da SegNet sulla Jetson Nano.}
    \label{segnet_result}
\end{figure}
Seppur non consigliata come piattaforma su cui effettuare il 
training di un modello, nella repository ufficiale \cite{repo_jetson_nano} viene riportato un link ad 
un'altra repository \cite{repo_pytorch_training} contenente il codice sorgente utile ad addestrare tutti 
i modelli impiegati nelle varie attività. Esiste una documentazione ufficiale 
contenente tutte le informazioni riguardanti gli script citati utilizzabili in 
ogni architettura presente nella famiglia Jetson \cite{Documentation_jetson}. Sia nel training che 
nell'inferenza di ogni modello, il framework di ML utilizzato è PyTorch.

\subsection{OpenCV}
Tra le tre architetture utilizzate in questo elaborato, una è sprovvista di una componente fondamentale utile ad accelerare l'allenamento e l'inferenza di un modello. I nuovi macbook pro, nonché il computer del sottoscritto, sono macchine che montano a bordo solo schede grafiche AMD. L'assenza di una scheda grafica NVidia implica l'utilizzo di librerie diverse da TensorRT il quale, a sua volta, fa uso delle librerie CUDA. Quando si verifica una tale limitazione, sia il training che l'inferenza vengono gestiti dalla componente principale di un computer: la CPU. Quando vi è un simile passaggio di elaborazione, le prestazioni computazionali sono nettamente inferiori rispetto a quelle ottenute su una scheda grafica. Ed eccoci giunti alla domanda principale: come può essere eseguito un modello su una piattaforma priva di una scheda grafica NVidia? A questo proposito ci viene in aiuto la nota libreria \emph{OpenCV}.
Principalmente utilizzata per lo sviluppo di applicazioni real-time rivolte alla visione artificiale e all'intelligenza artificiale. É in grado di fornire molteplici funzioni in grado di acquisire, analizzare e manipolare i dati visivi provenienti da uno specifica sorgete di input. Al suo interno, OpenCV possiede un modulo \emph{Deep Neural Network (DNN)} in grado di eseguire l'inferenza di un modello pre-addestrato direttamente su una CPU.
Questa libreria ha permesso di risolvere il problema inerente i test da svolgere sull'architettura Apple a disposizione. A partire dalla versione 4.2.0, OpenCV ha introdotto il pieno supporto alle schede grafiche GPU NVidia, sviluppando un nuovo modulo chiamato "\emph{cuDNN}".
Come ben si intuisce, gli autori hanno permesso di aumentare le performance utilizzando la capacità computazionale messa a disposizione di un hardware apposito. Tutto questo ha permesso di creare una sorta di accelerazione, lato inferenza, dei modelli pre-addestrati. Il supporto si è reso interessante per tutte quelle architetture che dispongono di una scheda grafica NVidia, prive di qualunque acceleratore come TensorRT. In questo elaborato, l'architettura che rientra in questa categoria è Google Colaboratory. A causa dell'impossibilità di installazione di TensorRT, si è preferito utilizzare il supporto messo a disposizione da OpenCV per poter ottenere i benchmarks voluti. Relativamente facile è risultata l'implementazione di questa funzionalità che ha permesso ad OpenCV di utilizzare la GPU messa a disposizione da Google. Le uniche due righe codice da poter aggiungere alla nostra rete (net) sono le seguenti:
\begin{itemize}
    \item \emph{net.setPreferableBackend(cv2.dnn.DNN\_BACKEND\_CUDA)}
    \item \emph{net.setPreferableTarget(cv2.dnn.DNN\_TARGET\_CUDA)}
\end{itemize}
Il resto del codice è uguale a quello utilizzato per l'inferenza su CPU.
Fondamentale risulta la conversione del modello pre-addestrato dal formato \emph{.pth} al formato \emph{.ONNX} per poter essere importato all'interno di OpenCV tramite la funzione \emph{readNetFromONNX()} messa sempre a disposizione nel modulo DNN.
Il supporto fornito da OpenCV ha permesso quindi di ottenere i benchmarks su entrambi le architetture utilizzate, ovvero sul macbook pro e su Google Colaboratory.

\section{Frames-per-Second (FPS)}
La velocità di inferenza rappresenta un indicatore di performance di ogni 
modello. Per poterla calcolare, la velocità di inferenza è rappresentata dai 
\emph{Frames-per-Second (FPS)} (\ref{FPS_Count}):
\begin{equation}\label{FPS_Count}
    FPS = \frac{1}{Ending \ Time - Starting \ Time}
\end{equation}
Questa misura è variabile e serve per rappresentare tre diversi elementi:
\begin{itemize}
    \item {\bfseries{\emph{Input}}}: ogni rete prende in input una sequenza di frame appartenenti 
    a un video/immagini. Ogni sequenza può avere un numero di FPS variabile. In 
    questo elaborato, vengono testati diversi video a 30FPS e a 60FPS.
    \item {\bfseries{\emph{Netwrok}}}: la velocità che la rete impiega ad effettuare uno specifico 
    task, può essere rappresentata dal numero di FPS. Riconoscere e/o 
    segmentare un oggetto appare essere un'attività onerosa in termini 
    computazionali, pertanto una rete è considerata veloce se, oltre a 
    produrre un output adeguato, svolge ogni task in un tempo breve.
    \item {\bfseries{\emph{Output}}}: il risultato prodotto da una rete è visibile solamente a 
    schermo. I video/immagini mostrati/e hanno una velocità di riproduzione 
    che è influenzata dal numero di FPS.
\end{itemize}
Tra le API messe a disposizione da NVidia, citate nella sezione \ref{utils}, 
fondamentale è risultato l'utilizzo dei metodi incaricati di calcolare la 
velocità d'inferenza, di input e di output raggiunta da ogni modello su ogni 
attività richiesta. 